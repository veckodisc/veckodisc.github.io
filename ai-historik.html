<!DOCTYPE html>
<html lang="sv">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI - Historik</title>
    <link rel="stylesheet" href="css/teknik.css">
</head>
<body>
<div class="container">
	<div class="flex-container">
		<div style="text-align: left"><h1>AI - Historik</h1></div>
		<div><h1 style="text-align: right; cursor: pointer" onclick="location.href='teknik-planering'">Till planeringen >></h1></div>
	</div>
	
    <h3>1940-talet</h3>
    <p>Under 1940-talet utvecklas <b>de första datorerna</b>. Då föds också tanken på att skapa ett <b>datorprogram som efterliknar
den mänskliga hjärnan</b>. Kunskap om neuroner och de synapser som kopplar samman dessa gav forskarna idéer om hur ett
datorprogram som efterliknade den mänskliga hjärnan kunde byggas upp. Men idén var under lång tid svår att förverkliga, delvis på grund av begränsad hårdvara, delvis på grund av brist på effektiva algoritmer för hur nätverken skulle vara sammankopplade och tränas upp.</p>

	<h3>1950</h3>
    <p>1950 formulerade matematikern Alan Turing det så kallade <b>Turingtestet</b> som är ett test av datorers intelligens. Turing menade att om en människa pratar med en dator och inte kan avgöra om det är ett program eller en människa som svarar, uppfylls kriteriet för mänsklig intelligens. Därmed har datorprogrammet klarat Turingtestet.<br><br>Idag har fortfarande inget AI-program klarat testet, men kanske är det inte långt kvar?</p>

    <h3>1950-60-talet</h3>
    <p>Termen <b>Artificiell Intelligens</b> myntades inför en sommarworkshop på Dartmouth College i USA 1956. 1950-60-talet präglades av en stor teknikoptimism, där forskare och finansiärer fick överdrivna förhoppningar om vad AI-tekniken skulle kunna prestera. Många problem visade sig dock vara betydligt svårare att lösa än vad man trott, såsom till exempel maskinöversättning från ett språk till ett annat.</p>
    <h3>1970-80-talet</h3>
    <p>Under 70-talet rådde en så kallad <b>AI-vinter</b>, där både Europa och USA drog ner på forskningsmedel för AI-forskning. I början av 1980-talet ökade AI-satsningar världen över igen, och gav upphov till det som kallas <b>expertsystem</b>, särskild hårdvara och mjukvara framtagna för ett specifikt ändamål, till exempel som beslutsstöd inom medicinsk diagnostik, planering och hårdvarudesign. I slutet av 80-talet <b>förbättrades de
neurala nätverkens förmåga</b> att lära sig av exempel, men idéerna om datorprogram som efterliknar hjärnans nätverk av neuroner var under denna period mer teoretiskt intressanta än praktiskt användbara.</p>
    <h3>1997</h3>
    <p>1997 uppnåddes en viktig milstolpe för expertsystemen, då IBMs expertsystem <b>Deep Blue lyckades besegra schackmästaren Kasparov</b>. Denna framgång gjorde allmänheten medvetna om vad AI kan åstadkomma. Deep Blue använde sig inte av maskininlärning. En viktig orsak till att Deep Blue kunde besegra schackmästaren var att <b>datorerna hade blivit tillräckligt kraftfulla</b> för att undersöka en stor mängd schackdrag på kort tid.</p>
    <h3>2000-talet</h3>
    <p>Att träna ett <b>neuralt nätverk kräver mycket stor beräkningskraft</b> och tidigare datorer var inte tillräckligt snabba och hade inte tillräckligt mycket minne. Det var först på 2000-talet som datorernas kapacitet räckte till för att bearbeta större datamängder. Med snabba datorer, och de speciella grafikprocessorerna som utvecklats för att styra bildskärmar, kunde man sätta igång den utveckling som nu går med rasande fart. Nya beräkningsmetoder och algoritmer för att träna datorerna har skapats, men i grunden bygger många av metoderna på de artificiella neurala nätverk som togs fram redan på 1980-talet.</p>
    <h3>2012</h3>
    <p>Ett stort utvecklingssteg skedde under 2010-talet. ImageNet Challenge var en klassificeringstävling där målet var att automatiskt klassificera bilder så korrekt som möjligt. År 2012 ställde ett lag upp med AlexNet, ett system baserat på <b>djupinlärning</b>, där djupet avser att metoden använder ett neuralt nätverk som består av många lager. AlexNet presterade väsentligt bättre än konkurrenterna och året därpå använde samtliga toppresterande bidrag olika varianter av djupinlärning. AlexNets framgång var genombrottet för deep learning (DL) och startskottet för den våg av AI-applikationer vi ser idag.<br><br>En liknande händelse som inträffade 2012 med koppling till det banbrytande framsteget inom deep learning och neurala nätverk var när forskare vid Google Brain använde ett djupt neuralt nätverk för bildigenkänning. Forskarna gav nätverket en stor mängd bilder från internet och lät det självt lära sig att känna igen mönster och objekt. När nätverket var vältränat upptäckte forskarna att en punkt i nätverket aktiverades av bilder på katter. Trots att ingen hade programmerat nätverket att hitta katter och trots att ingen av bilderna var märkta med information om att de innehöll katter, så hade nätverket lärt sig att identifiera katter helt på egen hand.<br><br>Att nätverken nu kunde träna sig på den <b>stora mängden data som finns tillgänglig på internet</b>, utan att datan behövde märkas upp, var en viktig orsak till den fortsatta AI-utvecklingen.</p>
    <h3>2017</h3>
    <p>2017 utvecklades två <b>nya metoder för att organisera och träna djupa neurala nätverk</b> som kraftigt förbättrade datorers förmåga att skapa bilder samt "förstå" och skapa ny text. Dessa metoder ligger till grund för dagens verktyg för att skapa bilder (t.ex. Midjourney och Dall-E) samt för att tolka och skapa nya texter (t.ex. ChatGPT).<br><br>Metoden som utvecklades för att hantera språk benämns <b>Transformer</b> och gör datorn mycket bättre på att "förstå" sammanhang och mer abstrakta resonemang.</p>
    <h3>2022</h3>
    <p>I slutet av 2022 lanserades <b>ChatGPT</b> av OpenAI. Den fick snabbt uppmärksamhet för att kunna svara på frågor inom många olika områden. På samma sätt som Deep Blue gjorde allmänheten medveten om vad AI kan åstadkomma, gjorde ChatGPT och AI-verktygen för att skapa bilder att allmänheten fick upp ögonen för AI:s förmåga. Till skillnad från Depp Blue är dagens kraftfulla verktyg tillgängliga för alla med en internetuppkoppling, till stor del helt gratis dessutom!<br><br>Många bitar har nu fallit på plats för att utveckla kraftfull AI:<br>&nbsp;&nbsp;•  Det finns en mycket <b>stor mängd träningsdata</b> på internet.<br>&nbsp;&nbsp;•  Det finns <b>stor kunskap om</b> hur <b>neurala nätverk</b> ska kopplas ihop och tränas.<br>&nbsp;&nbsp;•  <b>Datorerna</b> är tillräckligt <b>kraftfulla</b> för att kunna träna stora nätverk på mycket data.<br>&nbsp;&nbsp;•  Det görs <b>stora investeringar</b> i AI-tekniken.<br><br>Som exempel har version 3 av ChatGPT tränats på c:a 500 miljarder texter för att ställa in ungefär 175 miljarder kopplingar i det neurala nätverket, med hjälp av superdatorer som har mer än 285 000 processorkärnor.</p>
    
    <h3>Frivillig fördjupning</h3><p>AI-historik<br><a href="https://www.foi.se/rest-api/report/FOI%20Memo%207807">Sida 7-8 i den här texten från FOI   →</a><br><br>ChatGPT<br><a href="https://sv.wikipedia.org/wiki/ChatGPT">Wikipedia om ChatGPT   →</a></p>
</div>
</body>
</html>

