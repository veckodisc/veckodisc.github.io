<!DOCTYPE html>
<html lang="sv">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI - Historik</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #fcfcfc;
            text-align: center;
            padding: 20px;
            max-width: 800px;
            margin: 0 auto;
            line-height: 1.6; /* Ökat avstånd mellan raderna */
        }
        h3 {
            color: #007baa;
            text-align: left;
        }
        
        p {
            font-size: 16px;
            color: #333;
            text-align: left;
            margin-bottom: 50px;
        }
        ul {
            font-size: 16px;
            color: #333;
            text-align: left;
            margin-bottom: 50px;
            margin-top: -30px;
        }
    </style>
</head>
<body>
    <h3>1940-talet</h3>
    <p>Under 1940-talet utvecklas de första datornerna. Då föds också tanken på att skapa ett datorprogram som efterliknar
den mänskliga hjärnan. Kunskap om neuroner och de synapser som kopplar samman dessa gav forskarna idéer om hur ett
datorprogram som efterliknade den mänskliga hjärnan kunde byggas upp. Men idén var under lång tid svår att förverkliga, delvis på grund av begränsad hårdvara, delvis på grund av brist på effektiva algoritmer för hur nätverken skulle vara sammankopplade och arbeta.</p>
    <h3>1956</h3>
    <p>Termen ”Artificiell Intelligens” myntades inför en sommarworkshop på Dartmouth College i USA 1956. 1950-60-talet präglades av en stor teknikoptimism, där forskare och finansiärer fick överdrivna förhoppningar om vad AI-tekniken skulle kunna prestera, men många problem visade sig vara betydligt svårare att lösa än vad man trott, såsom till exempel maskinöversättning från ett språk till ett annat.</p>
    <h3>70-80-talet</h3>
    <p>Under 70-talet rådde en så kallad AI-vinter, där både Europa och USA drog ner på forskningsmedel för AI-forskning. I början av 1980-talet ökade AI-satsningar världen över igen, och gav upphov till det som kallas expertsystem, särskild hårdvara och mjukvara framtagna för ett specifikt ändamål, till exempel som beslutsstöd inom medicinsk diagnostik, planering och hårdvarudesign. I slutet av 80-talet förbättrades de
neurala nätverkens förmåga att lära sig av exempel, men idén om datorprogram som efterliknar hjärnans nätverk av neuroner var mer teoretiskt intressanta än praktiskt användbara.</p>
    <h3>1997</h3>
    <p>1997 uppnåddes en viktig milstolpe för expertsystemen, då IBMs expertsystem Deep Blue lyckades besegra schackmästaren Kasparov. Denna framgång gjorde allmänheten medvetna om vad AI kan åstadkomma. Deep Blue använde sig inte av maskininlärning. En viktig orsak till att Deep Blue kunde besegra schackmästaren var att datorerna hade blivit tillräckligt kraftfulla för att undersöka en stor mängd schackdrag på kort tid.</p>
    <h3>2000-talet</h3>
    <p>Att träna ett neuralt nätverk kräver mycket stor beräkningskraft och tidigare datorer var inte tillräckligt snabba och hade inte tillräckligt mycket minne. Det var först på 2000-talet som datorernas kapacitet räckte till för att bearbeta större datamängder. Med snabba datorer, och de speciella grafikprocessorersom utvecklats för att styra bildskärmar, kunde man sätta igång den utveckling som nu går med rasande fart. Nya beräkningsmetoder och algoritmer för att träna datorerna har skapats, men i grunden bygger många av metoderna på de artificiella neurala nät som togs fram redan på 1980-talet.</p>
    <h3>2012</h3>
    <p>Ett stort utvecklingssteg skedde under 2010-talet. ImageNet Challenge var en klassificeringstävling där målet var att automatiskt klassificera bilder så korrekt som möjligt. År 2012 ställde ett lag upp med AlexNet, ett system baserat på djupinlärning, där djupet avser att metoden använder ett neuralt nätverk som består av många lager. AlexNet presterade väsentligt bättre än konkurrenterna och året därpå använde samtliga toppresterande bidrag olika varianter av djupinlärning. AlexNets framgång var genombrottet för deep learning (DL) och startskottet för den våg av AI-applikationer vi ser idag.<br><br>En likanande händelse som inträffade 2012 med koppling till det banbrytande framsteget inom deep learning och neurala nätverk var när forskare vid Google Brain använde ett djupt neuralt nätverk för bildigenkänning. Forskarna gav nätverket en stor mängd bilder från internet och lät det självt lära sig att känna igen mönster och objekt. När nätverket var vältränat upptäckte forskarna att en punkt i nätverket aktiverades av bilder på katter. Trots att ingen hade programmerat nätverket att hitta katter och trots att ingen av bilderna var märkta med information om att de innehöll katter, så hade nätverket lärt sig att identifiera katter helt på egen hand.<br><br>Att nätverken nu kunde träna sig på den stora mängden data som finns tillgänglig på internet, utan att datan behövde märkas upp, var en viktig orsak till den fortsatta AI-utvecklingen.</p>
    <h3>2017</h3>
    <p>2017 utvecklas två nya metoder för att organisera och träna djupa neurala nätverk som får stor betydelse för datorers förmåga att skapa bilder samt "förstå" och skapa ny text. Dessa metoder ligger till grund för dagens verktyg för att skapa bilder (t.ex. Midjourney och Dall-E) samt för att tolka och skapa nya texter (t.ex. ChatGPT).<br><br>Metoden som utvecklades för att hantera språk benämns Transformer och gör datorn mycket bättre på att "förstå" sammanhang och mer abstrakta resonemang.</p>
    <h3>2022</h3>
    <p>I slutet av 2022 lanseras ChatGPT av OpenAI. Den fick snabbt uppmärksamhet för att kunna svara på frågor inom många olika områden. På samma sätt som Deep Blue gjorde allmänheten medveten om vad AI kan åstadkomma, gjorde ChatGPT och AI-verktygen för att skapa bilder att allmänheten fick upp ögonen för AI:s förmåga. Till skillnad från Depp Blue är dagens kraftfulla verktyg tillgängliga för alla med en internetuppkoppling, till stor del helt gratis dessutom!<br><br>Många bitar har nu fallit på plats för att utveckla kraftfull AI:<br>&nbsp;&nbsp;•  Det finns en mycket stor mängd träningsdata på internet.<br>&nbsp;&nbsp;•  Det finns stor kunskap om hur neurala nätverk ska kopplas ihop och tränas.<br>&nbsp;&nbsp;•  Datorerna är tillräckligt kraftfulla för att kunna träna stora nätverk på mycket data.<br>&nbsp;&nbsp;•  Det görs stora investeringar i AI-tekniken.<br><br>Som exempel har version 3 av ChatGPT tränats på c:a 500 miljarder texter för att ställa in ungefär 175 miljarder parametrar i det neurala nätverket, med hjälp av superdatorer som har mer än 285 000 processorkärnor.</p>
    
    <p><b>Frivillig fördjupning</b><br>AI-historik<br><a href="https://www.foi.se/rest-api/report/FOI%20Memo%207807">Sida 7-8 i den här texten från FOI</a><br><br>ChatGPT<br><a href="https://sv.wikipedia.org/wiki/ChatGPT">Wikipedia om ChatGPT</a>
</body>
</html>

